---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 16"
author: "Weicheng Hua, Emil Johannese Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
```

<!--  Etc (load all packages needed). -->

# Problem 1

## a)
The expected MSE on the test set is given by:
$$
\begin{aligned}
  E[(y_0 - \hat{f}(x_0))^2] 
  &= E[(f(x_0) - \hat{f}(x_0) + \epsilon)^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + 2E[\epsilon(f(x_0) - \hat{f}(x_0))] + E[\epsilon^2] \\
  &= \left \{E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] \right \}+ \left \{E[\hat{f}(x_0)^2]\right \}  + E[\epsilon^2]\\
  &= \left \{E[f(x_0)]^2 - 2E[f(x_0) \hat{f}(x_0)] \mathbf{\ + E[\hat{f}(x_0)]^2}\right \} 
  + \left \{ E[\hat{f}(x_0)^2] \mathbf{\ - E[\hat{f}(x_0)]^2}\right \} + E[\epsilon^2] \\
  &= \left \{E[f(x_0) - \hat{f}(x_0)]^2\right \}  + \left\{ E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2\right \} + E[\epsilon^2]\\
  &= E[f(x_0) - \hat{f}(x_0)]^2 + Var[\hat{f}(x_0)] + Var[\epsilon]\\
  &= \text{Squared bias} + \text{Variance of prediction} + \text{Irreducible error}\\ 
\end{aligned}
$$

## b)

The squared bias term represents the expected squared deviation between the prediction of the "true" model and the prediction of the fitted model. The variance of prediction term represents the degree to which the prediction of the fitted model can vary depending on the input. Higher variance of prediction means the model can adapt it's prediction to input data to a greater extent than a simpler model, implying that the model is more flexible. However, the increased "adaptability" may be unwanted if it leads to overfitting.

## c)
$$
\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}
$$

## d)

$$
\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}
$$

## e)

Answer: iii) 0.76

# Problem 2

```{r, eval=TRUE}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
head(penguins)
```

## a)
### 1)
Basel has not visualized the data prior to fitting the model, and has instead relied on "expert knowledge" to
fit the model. This has resulted in Basel dropping the bill length covariate from the model despite 
not having investigated it's significance in the first place.


### 2)
Basel has not understood the meaning of p-values. He has excluded the sex covariate as he mentioned that it has the smallest p-value. However, a small p-value may indicate that the sex covariate is significant in determining the body mass of penguins. In any case, an F-test should be conducted to determine whether the sex covariate should be omitted or kept, instead of looking directly at the p-value for the sex coefficient in the full model.   


### 3)
Basel concludes that chinstrap penguins have the largest body mass from the fact that the coefficient for chinstrap penguins has the largest value. However, he does not consider that the only negative interaction coefficient in the model is the coefficient for the interaction between bill depth and Chinstrap. The final body mass for chinstraps can therefore be lower than the other species due to this interaction coefficient.

## b)
```{r}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
# Remove island, and year variable, as we won't use those.
Penguins <- na.omit(subset(penguins, select = -c(island, year)))
boxplot(body_mass_g ~ sex, data=Penguins)
```
Judging from the box-plot above, the sex factor appears to be significant, 
with males having a larger average body mass than females. 

```{r}
boxplot(body_mass_g ~ species, data=Penguins)
```
Contrary to Basel's claim, we see that Gentoo is the species with the largest average body mass, not Chinstrap.

## c)
Original model
```{r}
# Fit the model as specified in advance based on "expert" knowledge:
penguin.model.initial <- lm(body_mass_g ~ flipper_length_mm  + sex + bill_depth_mm * species, 
                    data = Penguins)
summary(penguin.model.initial)
```
Basel's model
```{r}
penguin.model.basel <- lm(body_mass_g ~ flipper_length_mm + bill_depth_mm*species, 
                  data = Penguins)
summary(penguin.model.basel)
```
Improved model
```{r}
# Fit model with sex
penguin.model.improved <- lm(body_mass_g ~ flipper_length_mm + species + bill_length_mm + bill_depth_mm + sex, data = Penguins)
summary(penguin.model.improved)
```

```{r}
#F-test between initial model and Basel's model
anova(penguin.model.initial, penguin.model.basel)
```
Wee see from the p-value (< 2.2e-16) that the sex covariate appears 
to be highly significant , and that Basil's model got worse 
(SSE increased by 7736349) by excluding it from the model.
```{r}
#F-Test between initial model and 
anova(penguin.model.improved, penguin.model.initial)
```

```{r}
# Fit model without sex
penguin.model.improved.red <- lm(body_mass_g ~ flipper_length_mm  + bill_length_mm + species + bill_depth_mm, data = Penguins)
# Compare
anova(penguin.model.improved, penguin.model.improved.red)
```
Once again we observe that the model 

#F-test between our improved model and the initial model


In the initial model, a linear regression with body mass as the response, and flipper length, bill depth, species and sex as covariates, as well as an interaction effect between bill depth and species. An F-test was conducted between the original model with all covariates and basel's model resulting in a F-value of 34.5 and Pr(>F) of 1.049e-08. This indicates that the covariate terms that Basel have eliminated were actually quite important and should be included. Looking back at the original model, the covariate for bill length have been completely ignored by Basel and was included in the improved model. The interaction term between species and bill depth has the highest p-value and therefore a model without the interaction term was considered. An F-test was conducted to check the importance of the interaction term between billdepth and species. This gave a F-value of 5.1625 and a Pr(>F) of 0.006193. Though this might mean that the interaction term between billdepth and species have some significance, it is the least significant and have therefore been chosen to be omitted for simplicity. The final model without the interaction term have low p-values for all included covariates indicating that all covariates have significance. Based on the dummy variable for Gentoo penguins being the largest (657), and no species interaction term, the Gentoo penguins have in general the largest mass. This matches the box plots generated in part b). However this is also done assuming that the other covariates, bill_depth and flipper_length, are independent of species, which might not the case. The final model is therefore given by the equations.    

$$
\begin{aligned}
  \hat{y}_{adelie} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} +      \hat\beta_{bill\_depth} x_{bill\_depth} + \hat\beta_{bill\_length} x_{bill\_length}\\
  \hat{y}_{chinstrap} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} + \hat\beta_{bill\_length} x_{bill\_length} +\hat\beta_{bill\_depth} x_{bill\_depth} +\hat\beta_{chinstrap} \\
\hat{y}_{gentoo} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} + \hat\beta_{bill\_length} x_{bill\_length} + \hat\beta_{bill\_depth} x_{bill\_depth} +\hat\beta_{gentoo} 
\end{aligned}
$$
The 95% confidence window for each of the covariate coefficient is also given below. 
#95% confidence interval of our model
```{r}
confint(penguin.model.improved)
```


A Tukey-Anscombe plot to evaluate the fit of the model is generated below. No clear correlation can be seen between the residuals and fitted values, indicating that the expected value of the residual is 0 and that they all have the same variance.
```{r fig.dim = c(10,10)}
library(sfsmisc)
TA.plot( penguin.model.improved, fit = fitted(penguin.model.improved), res = residuals(penguin.model.improved), labels = "*" )

```


# Problem 3

Load and prepare data the problem 3

```{r}
# Load libraries
library(tidyverse)
library(class)
library(MASS)
library(palmerpenguins)
library(dplyr)
library(caret)
library(pROC)

# Load penguins data
Penguins <- penguins

# Add binary variable: if adelie
Penguins$adelie <- ifelse(Penguins$species == "Adelie", 1, 0)

# Extract just needed variables and remove na
Penguins_reduced <- Penguins %>% dplyr::select(body_mass_g, flipper_length_mm, adelie) %>% 
  mutate(body_mass_g = as.numeric(body_mass_g),
         flipper_length_mm = as.numeric(flipper_length_mm)) %>% 
  drop_na()

# Set seed for sample extraction
set.seed(4268)

# Find size of training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))

# Split data randomly into train and test
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)

train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

## a)

# i)

```{r}
# Make logistic regression model
model.LR <- glm(adelie ~ body_mass_g + flipper_length_mm,
                 family="binomial",
                 data=train)

# Get classification probabilities
pred.LR.prob <- model.LR %>% predict(test, type="response")

# Classify on test set using 0.5 cutoff
pred.LR <- ifelse(pred.LR.prob > 0.5, 1, 0)
```

# ii)

```{r}
# Make quadratic discriminant analysis model
model.QDA <- qda(adelie ~ body_mass_g + flipper_length_mm,
                 data=train)

# Get classification probabilities
pred.QDA.prob<- (model.QDA %>% predict(test))$posterior[0:nrow(test)]

# Classify on test set using 0.5 cutoff
pred.QDA <- ifelse(pred.QDA.prob > 0.5, 1, 0)
```

# iii)

```{r}
# K-nearest neighbor
model.KNN <- knn(train = train,
                 test = test,
                 cl = train$adelie,
                 k = 25,
                 prob = T)

# Get classification probabilities
pred.KNN.prob <- ifelse(model.KNN == 0, 1 - attributes(model.KNN)$prob, attributes(model.KNN)$prob)

# Classify on test set
pred.KNN <- model.KNN[0:nrow(test)]
```

# iv)

```{r}
# Calculate specificity and sensitivity
print("Sensitivity for logistic regression:")
sensitivity(as.factor(pred.LR), as.factor(test$adelie))
print("Sensitivity for QDA:")
sensitivity(as.factor(pred.QDA), as.factor(test$adelie))
print("Sensitivity for KNN:")
sensitivity(as.factor(pred.KNN), as.factor(test$adelie))

print("Specificity for logistic regression:")
specificity(as.factor(pred.LR), as.factor(test$adelie))
print("Specificity for QDA:")
specificity(as.factor(pred.QDA), as.factor(test$adelie))
print("Specificity for KNN:")
specificity(as.factor(pred.KNN), as.factor(test$adelie))

```

## b)

# i)

```{r}
# ROC for different classifiers
roc.LR <- roc(test$adelie, pred.LR.prob)
roc.KNN <- roc(test$adelie, pred.KNN.prob)
roc.QDA <- roc(test$adelie, pred.QDA.prob)

roc.list <- list(
  "Logistic regression" = roc.LR,
  "KNN" = roc.KNN,
  "QDA" = roc.QDA)

# Plot ROC
ggroc(roc.list, aes = "col",legacy.axes = TRUE) + 
  geom_abline() + 
  theme_classic() +
  ggtitle("ROC") +
  labs(x = "1 - Specificity",
       y = "Sensitivity",
       col = "Model type")

# AUC for the classifiers
roc.LR$auc
roc.KNN$auc
roc.QDA$auc
```

# ii)

The ROC curve shows how the binary classifiers are performing with a varying threshold. The plot shows the true positive rate, or sensitivity, against the true negative rate, or 1 - specificity. If a classifier is on the black diagonal line it performs just as good as guessing randomly and if it lays above the diagonal line it performs better. This means that a classifier is better the further up in the left corner its ROC curve are. In this case the logistic regression and the QDA performs better than the KNN classifier.

The AUC, or area under the curve, is just the area under the ROC curve and provides an aggregate measure of all classifiers for all thresholds. As expected, the AUC for the logistic regression and the QDA are higher than the one for the KNN indicating better performance.

# iii)

We would say the logistic regression model is the most interpretable model. This model provides easy to understand betas telling about the realtionship between the body mass and flipper length and if the penguin is Adelie or not. The two other models does not provide the same information about the data.

## c)

```{r}
model.LR$coefficients
```

Increasing the body mass of penguin with 1000g will result in the following change in the odds

$$
exp(0.0007120 * 1000) = 2.038
$$

The odds increases by a factor of 2.038. Alternative iii.

## d)

```{r}
# Get all data, from both test and training
penguins.pred.all <- Penguins_reduced %>% dplyr::select(-adelie)

# Classify on both test and training data using LR model
pred.all <- ifelse(model.LR %>% predict(penguins.pred.all) > 0.5, 1, 0)

penguins.pred.all <- Penguins_reduced %>% dplyr::mutate(pred = pred.all)

# Plot result
ggplot(penguins.pred.all, aes(x=flipper_length_mm, y=body_mass_g)) + 
  ggtitle("Classification of penguins") +
  geom_point(aes(col = as.factor(adelie), shape = as.factor(pred))) + 
  labs(x = "Flipper length [mm]",
       y = "Body mass [g]",
       col = "True class",
       shape = "Predicted class") + 
  scale_colour_discrete(labels=c("Not Adelie", "Adelie")) +
  scale_shape_discrete(labels=c("Not Adelie", "Adelie"))
```

# Problem 4

## a)
(i) True. The process for validation set approach is only necessary to be repeated once but 10 times in 10-fold CV.
(ii)False. LOOCV has the highest variance as the datasets used between training are highly correlated and differ only by one observation which can lead to high variance between completely new datasets.
(iii)False. In the validation set-approach the data is split randomly into 2 equal sets with 1 set being the training set and the other being the validation set. In the 2-fold CV the data is again randomly split into 2 equal parts but each data set take turn being the training and validation set.
(iv) False. LOOCV is the most computationally expensive way to do cross-validation. 

## b) 
```{r}

id <- "1chRpybM5cJn4Eow3-_xwDKPKyddL9M2N" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
```

```{r}
m <- glm(family="binomial", formula = chd ~ sbp + sex + smoking, data=d.chd)
d.chd.new <- data.frame(
  sex=as.integer(c(1)), 
  sbp = as.numeric(c(150)), 
  smoking=as.integer(c(0)))
pred <- predict(m, d.chd.new, type="response")
pred
```
The probability of chd for a non-smoking male with sbp=150 is 10%.

## c)
### 1)
```{r}
set.seed(4268)
library(boot)

prob <- function(df, index){
  m <- glm(family="binomial", subset=index, formula = chd ~ sbp + sex + smoking, data=d.chd)
  return(predict(m, d.chd.new, type="response")) 
}
```

### 2)
```{r}
B <- 1000
boot.result <- boot(d.chd, prob, B)
boot.result
```
From the bootstrapping method we observe an estimated standard error of 0.044.

### 3)
```{r}
boot.ci(boot.result, 0.95)
```
From the bootstrapping method we obtain [0.0107, 0.1843] as
the first order normal approximation of the 95 % CI.

### 4)
Since the 95 % CI is wide (nearly approaching zero in the left-hand limit)
we conclude based on the bootstrapping computations that the conditional
probability of chd in a non-smoking male with sbp=150 is quite uncertain.
Plausible values for conditional chd probability thus lie in the interval
[0.0107, 0.1843].
The upper limit of .1843 is perhaps most useful as it provides an upper 
97.5 % confidence bound on estimated conditional chd risk.

## d)
$$
\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
FALSE & FALSE & TRUE & TRUE\\
\end{matrix}
$$
