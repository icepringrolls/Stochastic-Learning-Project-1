---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 16"
author: "Weicheng Hua, Emil Johannese Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("palmerpenguins")
# install.packages("ggfortify") # For model checking
# install.packages("MASS")
# install.packages("class")
# install.packages("pROC")
# install.packages("plotROC")
# install.packages("boot")
library("knitr")
library("rmarkdown")
library("palmerpenguins")
```

<!--  Etc (load all packages needed). -->

# Problem 1

## a)
The expected MSE on the test set is given by:
$$\begin{aligned}
  E[(y_0 - \hat{f}(x_0))^2] 
  &= E[(f(x_0) - \hat{f}(x_0) + \epsilon)^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + 2E[\epsilon(f(x_0) - \hat{f}(x_0))] + E[\epsilon^2] \\
  &= E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] + E[\hat{f}(x_0)^2] + E[\epsilon^2]\\
  &= E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] + E[\hat{f}(x_0)]^2 
  + ( E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2) + E[\epsilon^2] \\
  &= E[f(x_0) - \hat{f}(x_0)]^2 + ( E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2) + E[\epsilon^2]\\
  &= E[f(x_0) - \hat{f}(x_0)]^2 + Var[\hat{f}(x_0)] + Var[\epsilon]\\
  &= \text{Squared bias} + \text{Variance of prediction} + \text{Irreducible error}\\ 
\end{aligned}$$

## b)

The squared bias term represents the expected squared deviation between the prediction of the "true" model and the prediction of the fitted model. The variance of prediction term represents the degree to which the prediction of the fitted model can vary depending on the input. Higher variance of prediction means the model can adapt it's prediction to input data to a greater extent than a simpler model, implying that the model is more flexible. However, the increased "adaptability" may be unwanted if it leads to overfitting.

## c)

$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## d)

$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## e)

```{r}
library(matrixcalc)
mat <- cbind(c(50, 33, 18), c(33, 38, -10), c(18, -10, 72))
is.positive.semi.definite(mat)
```

Answer: iii) 0.76

# Problem 2

Here is a code chunk:

```{r, eval=TRUE}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
head(penguins)
```

## a)

  1)Basel have understood the meaning of p-values. He has excluded the sex covariate as he mentioned that it has the smallest p-value. However, a small p-value indicates that the  sex covariate has significant importance in determine the body mass of penguins. A F-test should be conducted before the choice of omitting the sex covariate.   
  2)The body  mass of penguins has been described to be dependent on the species of the penguin. This done despite the fact that species have the highest p-value in the original model. This is evident by an extremely high p-value for the species coefficients in the final model which suggests that there is a high probability for observing the null hypothesis, where there is no relationship between species and body mass.A F-test can be conducted to determine if species should be omitted. 
  3)Basel concludes that chinstrap penguins have the largest body mass from the fact that the dummy variable for chinstrap penguins have the largest value. However he does not consider that the interaction effect between bill depth and Chinstrap is the only negative term while the others are positive. The final body mass for chinstraps can therefore be lower than the other species due to this interaction term. Correlation between bill depth and species are not considered as well.
   

## b)
```{r, echo = FALSE, fig.dim = c(10,10)}
############## =^._.^=  ~~~BASIL'S CODE~~~  =^._.^= ############## 
#install.packages("palmerpenguins")  # Run if you haven't installed this before.


library(palmerpenguins) # Contains the data set "penguins".
data(penguins)

# Remove island, and year variable, as we won't use those.
Penguins <- subset(penguins, select = -c(island, year))

library(GGally)
gpairs_sex <- ggpairs(Penguins, mapping = ggplot2::aes(colour = sex))
primary_var <- "body_mass_g"
pvar_pos <- match(primary_var, gpairs_sex$xAxisLabels)
plots <- lapply(1:gpairs_sex$ncol, function(j) getPlot(gpairs_sex, i = pvar_pos, j = j))
ggmatrix(
    plots,
    nrow = 1,
    ncol = gpairs_sex$ncol,
    xAxisLabels = gpairs_sex$xAxisLabels,
    yAxisLabels = primary_var,
    title = "Body Mass to sex correlations"
)
gpairs_species <- ggpairs(Penguins, mapping = ggplot2::aes(colour = species))
primary_var <- "body_mass_g"
pvar_pos <- match(primary_var, gpairs_species$xAxisLabels)
plots <- lapply(1:gpairs_species$ncol, function(j) getPlot(gpairs_species, i = pvar_pos, j = j))
ggmatrix(
    plots,
    nrow = 1,
    ncol = gpairs_species$ncol,
    xAxisLabels = gpairs_species$xAxisLabels,
    yAxisLabels = primary_var,
    title = "Body Mass to species correlations"
)
```
The plots above show the factors for body mass first separated by sex and then by species. From the body mass to sex correlations plots, it can be seen that in general, male penguins have a higher body mass than females. Sex covariate should therefore be considered. From the species plot, it can be seen that Gentoo penguins in general have a higher body mass and therefore Basil's conclusion that Chinstrap penguins have the highest mass is incorrect. THe omission of bill length might also not be correct as there appears to be a correlation between bill length and body mass. Finally, Basel might not be right in his assessment that there is an interaction between the covariates species and bill depth. The slopes of the trend line between bill depth and body mass appears to remain constant regardless of the species. From the plots, there does not appear to be any interaction terms between sex and any other covariates.

Basel should probably have first get a rough estimation of the importance of various covariates for body mass by generating these plots to get an overall picture at least graphically



## c)

```{r}
############## =^._.^=  ~~~BASIL'S CODE~~~  =^._.^= ############## 
#install.packages("palmerpenguins")  # Run if you haven't installed this before.


library(palmerpenguins) # Contains the data set "penguins".
data(penguins)

# Remove island, and year variable, as we won't use those.
Penguins <- subset(penguins, select = -c(island, year))

# Fit the model as specified in advance based on expert knowledge:
penguin.model.initial <- lm(body_mass_g ~ flipper_length_mm  + bill_length_mm + bill_depth_mm*species, 
                    data = Penguins)


penguin.model.basel <- lm(body_mass_g ~ flipper_length_mm + bill_depth_mm*species, 
                  data = Penguins)

penguin.model.mine <- lm(body_mass_g ~ flipper_length_mm  + bill_length_mm + species +  bill_depth_mm, data = Penguins)


# Look at the model coefficients
summary(penguin.model.initial)$coefficients
summary(penguin.model.basel)$coefficients
summary(penguin.model.mine)$coefficients

# Fit final model without sex
summary(penguin.model.mine)

#F-test between initial model and Basel's final model
anova(final.model,penguin.model)

#F-test between initial model and our final model
anova(penguin.model,penguin.model.test)

#95% confidence interval of our model
confint(penguin.model.mine)



```

In the initial model, a linear regression with body mass as the response, and flipper length, bill depth, bill length, species and sex as covariates, as well as an interaction effect between bill depth and species. An F-test was conduceted between the original model with all covariates and basel's model resulting in a F-value of 34.5 and Pr(>F) of 1.049e-08. This indicates that the covariates term that Basel have eliminated are important and should be included. Looking back at the original model, the interaction term between species and bill depth has the highest p-value and therefore a model without the interaction term was generated. This theory was further evidenced by plots generated in part (b).An F-test was conducted to check the importance of the interaction term between billdepth and species. This gave a F-value of 5.1625 and a Pr(>F) of 0.006193. Though this might mean that the interaction term between billdepth and species have some significance, it is the least significant and have been chosen to be omitted for simplicity. The final model without the interaction term but all the other covairaites have low p-values for all covariates. Based on the dummy variable for Gentoo penguins being the larges (657), and no species interaction term the Gentoo penguins have in general the largest mass. However this is done assuming that the other covariates, bill_depth and flipper_length, are independent of species, which is not the case. The final model is therefore given by the equations.    

$$
\begin{aligned}
  \hat{y}_{adelie} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} +      \hat\beta_{bill\_depth} x_{bill\_depth} + \hat\beta_{bill\_length} x_{bill\_length}\\
  \hat{y}_{chinstrap} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} + \hat\beta_{bill\_length} x_{bill\_length} + \hat\beta_{chinstrap} \\
\hat{y}_{gentoo} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} + \hat\beta_{bill\_length} x_{bill\_length} + \hat\beta_{gentoo} 
\end{aligned}
$$
The 95% confidence window for each of the covariate coefficient is also given above. 


A plot to evaluate the fit for the model is generated below.
```{r fig.dim = c(10,10)}



```


# Problem 3
Load and prepare data the problem 3
```{r}
# Load libraries
library(tidyverse)
library(class)
library(MASS)
library(palmerpenguins)
library(dplyr)
library(caret)
library(pROC)

# Load penguins data
Penguins <- penguins

# Add binary variable: if adelie
Penguins$adelie <- ifelse(Penguins$species == "Adelie", 1, 0)

# Extract just needed variables and remove na
Penguins_reduced <- Penguins %>% dplyr::select(body_mass_g, flipper_length_mm, adelie) %>% 
  mutate(body_mass_g = as.numeric(body_mass_g),
         flipper_length_mm = as.numeric(flipper_length_mm)) %>% 
  drop_na()

# Set seed for sample extraction
set.seed(4268)

# Find size of training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))

# Split data randomly into train and test
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)

train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```
## a)
# i)
```{r}
# Make logistic regression model
model.LR <- glm(adelie ~ body_mass_g + flipper_length_mm,
                 family="binomial",
                 data=train)

# Get classification probabilities
pred.LR.prob <- model.LR %>% predict(test)

# Classify on test set using 0.5 cutoff
pred.LR <- ifelse(pred.LR.prob > 0.5, 1, 0)
```
# ii)
```{r}
# Make quadratic discriminant analysis model
model.QDA <- qda(adelie ~ body_mass_g + flipper_length_mm,
                 data=train)

# Get classification probabilities
pred.QDA.prob<- (model.QDA %>% predict(test))$posterior[0:nrow(test)]

# Classify on test set using 0.5 cutoff
pred.QDA <- ifelse(pred.QDA.prob > 0.5, 1, 0)
```

# iii)
```{r}
# K-nearest neighbor
model.KNN <- knn(train = train,
                 test = test,
                 cl = train$adelie,
                 k = 25,
                 prob = T)

# Get classification probabilities
pred.KNN.prob <- ifelse(model.KNN == 0, 1 - attributes(model.KNN)$prob, attributes(model.KNN)$prob)

# Classify on test set
pred.KNN <- model.KNN[0:nrow(test)]
```

# iv)
```{r}
# Calculate specificity and sensitivity
print("Sensitivity for logistic regression:")
sensitivity(as.factor(pred.LR), as.factor(test$adelie))
print("Sensitivity for QDA:")
sensitivity(as.factor(pred.QDA), as.factor(test$adelie))
print("Sensitivity for KNN:")
sensitivity(as.factor(pred.KNN), as.factor(test$adelie))

print("Specificity for logistic regression:")
specificity(as.factor(pred.LR), as.factor(test$adelie))
print("Specificity for QDA:")
specificity(as.factor(pred.QDA), as.factor(test$adelie))
print("Specificity for KNN:")
specificity(as.factor(pred.KNN), as.factor(test$adelie))

```

## b)

# i)
```{r}
# ROC for different classifiers
roc.LR <- roc(test$adelie, pred.LR.prob)
roc.KNN <- roc(test$adelie, pred.KNN.prob)
roc.QDA <- roc(test$adelie, pred.QDA.prob)

roc.list <- list(
  "Logistic regression" = roc.LR,
  "KNN" = roc.KNN,
  "QDA" = roc.QDA)

# Plot ROC
ggroc(roc.list, aes = "col",legacy.axes = TRUE) + 
  geom_abline() + 
  theme_classic() +
  ggtitle("ROC") +
  labs(x = "1 - Specificity",
       y = "Sensitivity",
       col = "Model type")

# AUC for the classifiers
roc.LR$auc
roc.KNN$auc
roc.QDA$auc
```
# ii)
The ROC curve shows how the binary classifiers are performing with a varying threshold. The plot shows the true positive rate, or sensitivity, against the true negative rate, or 1 - specificity. If a classifier is on the black diagonal line it performs just as good as guessing randomly and if it lays above the diagonal line it performs better. This means that a classifier is better the further up in the left corner its ROC curve are. In this case the logistic regression and the QDA performs better than the KNN classifier.

The AUC, or area under the curve, is just the area under the ROC curve and provides an aggregate measure of all classifiers for all thresholds. As expected, the AUC for the logistic regression and the QDA are higher than the one for the KNN indicating better performance. 

# iii)
We would say the logistic regression model is the most interpretable model. This model provides easy to understand betas telling about the realtionship between the body mass and flipper length and if the penguin is Adelie or not. The two other models does not provide the same information about the data.


## c)
```{r}
model.LR$coefficients
```
Increasing the body mass of penguin with 1000g will result in the following change in the oddds

$$ 
\begin{equation}

exp(0.0007120 * 1000) = 2.028

\end{equation}
$$
The odds increases by a factor of 2.028.


## d)
```{r}
# Get all data, from both test and training
penguins.pred.all <- Penguins_reduced %>% dplyr::select(-adelie)

# Classify on both test and training data using LR model
pred.all <- ifelse(model.LR %>% predict(penguins.pred.all) > 0.5, 1, 0)

penguins.pred.all <- Penguins_reduced %>% dplyr::mutate(pred = pred.all)

# Plot result
ggplot(penguins.pred.all, aes(x=flipper_length_mm, y=body_mass_g)) + 
  ggtitle("Classification of penguins") +
  geom_point(aes(col = as.factor(adelie), shape = as.factor(pred))) + 
  labs(x = "Flipper length [mm]",
       y = "Body mass [g]",
       col = "True class",
       shape = "Predicted class") + 
  scale_colour_discrete(labels=c("Not Adelie", "Adelie")) +
  scale_shape_discrete(labels=c("Not Adelie", "Adelie"))
```


# Problem 4
