---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 16"
author: "Weicheng Hua, Emil Johannese Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("palmerpenguins")
# install.packages("ggfortify") # For model checking
# install.packages("MASS")
# install.packages("class")
# install.packages("pROC")
# install.packages("plotROC")
# install.packages("boot")
library("knitr")
library("rmarkdown")
library("palmerpenguins")
```

<!--  Etc (load all packages needed). -->


# Problem 1

## a)
The expected MSE on the test set is given by:
$$\begin{aligned}
  E[(y_0 - \hat{f}(x_0))^2] &= 
  E[(f(x_0) - \hat{f}(x_0) + \epsilon)^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + 2E[\epsilon(f(x_0) - \hat{f}(x_0))] + E[\epsilon^2] \\
  &= E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] + E[\hat{f}(x_0)^2] + E[\epsilon^2]\\
  &= E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0) + \hat{f}(x_0)^2] 
  + ( E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2) + E[\epsilon^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + ( E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2) + E[\epsilon^2]\\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + Var[\hat{f}(x_0)] + Var[\epsilon]\\
  &= \text{Squared bias} + \text{Variance of prediction} + \text{Irreducible error}\\ 
\end{aligned}$$

## b)
The squared bias term represents the expected squared deviation between the prediction of the 
"true" model and the prediction of the fitted model.
The variance of prediction term represents the degree to which the prediction of
the fitted model can vary depending on the input. Higher variance of
prediction means the model can adapt it's prediction to input data to a greater extent
than a simpler model, implying that the model is more flexible. 
However, the increased "adaptability" may be unwanted if it leads to overfitting.

## c)
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## d)
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## e)
```{r}
library(matrixcalc)
mat <- cbind(c(50, 33, 18), c(33, 38, -10), c(18, -10, 72))
is.positive.semi.definite(mat)
```

Answer: iii) 0.76
 

# Problem 2

Here is a code chunk:

```{r, eval=TRUE}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
head(penguins)
```


## a)

## b) 
 
## c)


# Problem 3

# Problem 4
 