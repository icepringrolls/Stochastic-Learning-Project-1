---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 16"
author: "Weicheng Hua, Emil Johannese Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") #probably already installed
# install.packages("rmarkdown") #probably already installed
# install.packages("ggplot2") #plotting with ggplot
# install.packages("palmerpenguins")
# install.packages("ggfortify") # For model checking
# install.packages("MASS")
# install.packages("class")
# install.packages("pROC")
# install.packages("plotROC")
# install.packages("boot")
library("knitr")
library("rmarkdown")
library("palmerpenguins")
```

<!--  Etc (load all packages needed). -->


# Problem 1

## a)
The expected MSE on the test set is given by:
$$\begin{aligned}
  E[(y_0 - \hat{f}(x_0))^2] 
  &= E[(f(x_0) - \hat{f}(x_0) + \epsilon)^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + 2E[\epsilon(f(x_0) - \hat{f}(x_0))] + E[\epsilon^2] \\
  &= \left \{E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] \right \}+ \left \{E[\hat{f}(x_0)^2]\right \}  + E[\epsilon^2]\\
  &= \left \{E[f(x_0)]^2 - 2E[f(x_0) \hat{f}(x_0)] \mathbf{\ + E[\hat{f}(x_0)]^2}\right \} 
  + \left \{ E[\hat{f}(x_0)^2] \mathbf{\ - E[\hat{f}(x_0)]^2}\right \} + E[\epsilon^2] \\
  &= \left \{E[f(x_0) - \hat{f}(x_0)]^2\right \}  + \left\{ E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2\right \} + E[\epsilon^2]\\
  &= E[f(x_0) - \hat{f}(x_0)]^2 + Var[\hat{f}(x_0)] + Var[\epsilon]\\
  &= \text{Squared bias} + \text{Variance of prediction} + \text{Irreducible error}\\ 
\end{aligned}$$

## b)
The squared bias term represents the expected squared deviation between the prediction of the 
"true" model and the prediction of the fitted model.
The variance of prediction term represents the degree to which the prediction of
the fitted model can vary depending on the input. Higher variance of
prediction means the model can adapt it's prediction to input data to a greater extent
than a simpler model, implying that the model is more flexible. 
However, the increased "adaptability" may be unwanted if it leads to overfitting.

## c)
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## d)
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## e)
```{r}
library(matrixcalc)
mat <- cbind(c(50, 33, 18), c(33, 38, -10), c(18, -10, 72))
is.positive.semi.definite(mat)
```

Answer: iii) 0.76
 

# Problem 2

Here is a code chunk:

```{r, eval=TRUE}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
head(penguins)
```


## a)

## b) 
 
## c)


# Problem 3

# Problem 4

## a)
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & FALSE & FALSE\\
\end{matrix}$$

## b) 
```{r}

id <- "1chRpybM5cJn4Eow3-_xwDKPKyddL9M2N" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
```

```{r}
m <- glm(family="binomial", formula = chd ~ sbp + sex + smoking, data=d.chd)
d.chd.new <- data.frame(
  sex=as.integer(c(1)), 
  sbp = as.numeric(c(150)), 
  smoking=as.integer(c(0)))
pred <- predict(m, d.chd.new, type="response")
pred
```
The probability of chd for a non-smoking male with sbp=150 is 10%.

## c)
### 1)
```{r}
set.seed(4268)
library(boot)

prob <- function(df, index){
  m <- glm(family="binomial", subset=index, formula = chd ~ sbp + sex + smoking, data=d.chd)
  return(predict(m, d.chd.new, type="response")) 
}
```

### 2)
```{r}
B <- 1000
boot.result <- boot(d.chd, prob, B)
boot.result
```
From the bootstrapping method we observe an estimated standard error of 0.044.

### 3)
```{r}
boot.ci(boot.result, 0.95)
```
From the bootstrapping method we obtain [0.0107, 0.1843] as
the first order normal approximation of the 95 % CI.

### 4)
Since the 95 % CI is wide (nearly approaching zero in the left-hand limit)
we conclude based on the bootstrapping computations that the conditional
probability of chd in a non-smoking male with sbp=150 is quite uncertain.
Plausible values for conditional chd probability thus lie in the interval
[0.0107, 0.1843].
The upper limit of .1843 is perhaps most useful as it provides an upper 
97.5 % confidence bound on estimated conditional chd risk.

## d)
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
FALSE & FALSE & TRUE & TRUE\\
\end{matrix}$$
 