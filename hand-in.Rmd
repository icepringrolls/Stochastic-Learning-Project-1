---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 16"
author: "Weicheng Hua, Emil Johannese Haugstvedt, Torbj√∏rn Baadsvik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
```

<!--  Etc (load all packages needed). -->

# Problem 1

## a)
The expected MSE on the test set is given by:
$$
\begin{aligned}
  E[(y_0 - \hat{f}(x_0))^2] 
  &= E[(f(x_0) - \hat{f}(x_0) + \epsilon)^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + 2E[\epsilon(f(x_0) - \hat{f}(x_0))] + E[\epsilon^2] \\
  &= \left \{E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] \right \}+ \left \{E[\hat{f}(x_0)^2]\right \}  + E[\epsilon^2]\\
  &= \left \{E[f(x_0)]^2 - 2E[f(x_0) \hat{f}(x_0)] \mathbf{\ + E[\hat{f}(x_0)]^2}\right \} 
  + \left \{ E[\hat{f}(x_0)^2] \mathbf{\ - E[\hat{f}(x_0)]^2}\right \} + E[\epsilon^2] \\
  &= \left \{E[f(x_0) - \hat{f}(x_0)]^2\right \}  + \left\{ E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2\right \} + E[\epsilon^2]\\
  &= E[f(x_0) - \hat{f}(x_0)]^2 + Var[\hat{f}(x_0)] + Var[\epsilon]\\
  &= \text{Squared bias} + \text{Variance of prediction} + \text{Irreducible error}\\ 
\end{aligned}
$$

## b)

The squared bias term represents the expected squared deviation between the prediction of the "true" model and the prediction of the fitted model. The variance of prediction term represents the degree to which the prediction of the fitted model can vary depending on the input. Higher variance of prediction means the model can adapt it's prediction to input data to a greater extent than a simpler model, implying that the model is more flexible. However, the increased "adaptability" may be unwanted if it leads to overfitting.

## c)
$$
\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}
$$

## d)

$$
\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}
$$

## e)

Answer: iii) 0.76

# Problem 2

```{r, eval=TRUE}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
head(penguins)
```

## a)
### 1)
Basel has not visualized the data prior to fitting the model, and has instead relied on "expert knowledge" to
fit the model. This has resulted in Basel dropping the bill length covariate from the model despite 
not having investigated it's significance in the first place.


### 2)
Basel has not understood the meaning of p-values. He has excluded the sex covariate as he mentioned that it has the smallest p-value. However, a small p-value may indicate that the sex covariate is significant in determining the body mass of penguins. In any case, an F-test should be conducted to determine whether the sex covariate should be omitted or kept, instead of looking directly at the p-value for the sex coefficient in the full model.   


### 3)
Basel concludes that chinstrap penguins have the largest body mass from the fact that the coefficient for chinstrap penguins has the largest value. However, he does not consider that the only negative interaction coefficient in the model is the coefficient for the interaction between bill depth and Chinstrap. The final body mass for chinstraps can therefore be lower than the other species due to this interaction coefficient.

## b)
```{r}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
# Remove island, and year variable, as we won't use those.
Penguins <- na.omit(subset(penguins, select = -c(island, year)))
```


```{r}
library(patchwork)
boxplot1 <- ggplot(data=Penguins, mapping=aes(x=sex, y=body_mass_g)) + geom_boxplot() 
boxplot2 <- ggplot(data=Penguins, mapping=aes(x=species, y=body_mass_g)) + geom_boxplot() 
boxplot1 / boxplot2
```
Judging from the box-plot above, the sex factor appears to be significant, 
with males having a larger average body mass than females. 
Contrary to Basel's claim, we see that Gentoo is the species with the largest average body mass, not Chinstrap.
## c)

```{r}
library(GGally)
library(patchwork)

disc_plots <- function(){
  gpairs <- ggpairs(Penguins, mapping = aes(colour = sex))
  mass_pos <- match("body_mass_g", gpairs$xAxisLabels)
  getPlot(gpairs, i=mass_pos, j=1)
}

cont_plots <- function(colour){
  if(colour=="species"){
    gpairs <- ggpairs(Penguins, mapping = aes(colour = species))
  } else {
    gpairs <- ggpairs(Penguins, mapping = aes(colour = sex))
  }
  ind <- function(var_name){
    match(var_name, gpairs$xAxisLabels)
  }
  cont_vars <- c("bill_length_mm", "bill_depth_mm", "flipper_length_mm")
  cont_inds <- sapply(cont_vars, function(var) ind(var))
  mass_pos <- match("body_mass_g", gpairs$xAxisLabels)
  plots <- lapply(cont_inds, function(j) getPlot(gpairs, i = mass_pos, j = j))
  ggmatrix(
      plots,
      byrow=TRUE,
      nrow = 1,
      ncol = length(cont_inds),
      xAxisLabels = c(cont_vars),
      yAxisLabels = c("body_mass_g"),
      title = "Scatter plots",
      showStrips = TRUE,
      legend=3,
      gg=theme(axis.text=element_text(size=12))
  )
}

```
Plots of body-mass distributions for the different species and sexes:
```{r fig.dim = c(6,2)}
disc_plots()
```
From the plot above we observe that males on average have a larger body mass than females.
However, the most significant difference in body mass is found between the Gentoo species and
the two other species.

Scatter plot for each continuous variable categorized by species:
```{r fig.dim = c(6,2)}
cont_plots("species")
```

Scatter plot for each continuous variable categorized by sex:

```{r fig.dim = c(6,2)}
cont_plots("sex")
```
We observe from the two scatter plots that the most significant differences in
attributes are found between different species, not the sexes.
However, the sex scatter plot indicates that males tend to score higher 
in most attributes as the clusters are centered more upward and to the left than
females in the plot.
That being said, we see no clear evidence that the regression coefficient
for any of the 3 continuous variables differ significantly among species or sexes. 
This motivates us to disregard interactions.

Initial model
```{r}
# Fit the model as specified in advance based on "expert" knowledge:
penguin.model.initial <- lm(body_mass_g ~ flipper_length_mm  + sex + bill_depth_mm * species, 
                    data = Penguins)
```

Basel's model
```{r}
penguin.model.basel <- lm(body_mass_g ~ flipper_length_mm + bill_depth_mm*species, 
                  data = Penguins)
```

```{r}
#F-test between initial model and Basel's model
anova(penguin.model.initial, penguin.model.basel)
```
We see from the p-value (< 2.2e-16) that the sex covariate appears 
to be highly significant , and that Basils model got worse 
(SSE increased by 7736349) by excluding it from the model.

We begin by creating a simple model
```{r}
# Fit improved model without interactions
penguin.model.improved <- lm(body_mass_g ~ 
                               flipper_length_mm 
                             + bill_length_mm
                             + bill_depth_mm 
                             + species
                             + sex, 
                             data = Penguins)
summary(penguin.model.improved)$coefficient
```
We see that all coefficients in the simpler model appear significant with a p-value below 0.05.

```{r}

# Fit improved model wit interaction between bill depth and species
penguin.model.improved2 <- lm(body_mass_g ~ 
                                flipper_length_mm
                              + bill_length_mm
                              + bill_depth_mm
                              * species
                              + sex, 
                              data = Penguins)
summary(penguin.model.improved2)$coefficient

```
The coefficient for the interaction between bill_depth and speciesGentoo 
appears to be insignificant with p-value > 0.5. 
```{r}
# compare models:
anova(penguin.model.improved, penguin.model.improved2)
```
The F-test indicates that the interactions may be significant with a p-value of 0.0072.
However due to the large p-value for interaction coefficient mentioned before, 
we elect to keep the simpler model.
We conduct an F-test to compare our improved model with the initial model:
```{r}
#F-Test between improved model and initial model 
anova(penguin.model.improved, penguin.model.initial)
```
Although the F-test suggests an insignificant difference in performance for the two models,
we still prefer our model since it does not include interaction terms.
A summary of the improved model is given below:
```{r}
summary(penguin.model.improved)
```

The final model is given by the following equations.    

$$
\begin{aligned}
  \hat{y}_{adelie} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} +      \hat\beta_{bill\_depth} x_{bill\_depth} + \hat\beta_{bill\_length} x_{bill\_length}\\
  \hat{y}_{chinstrap} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} + \hat\beta_{bill\_length} x_{bill\_length} +\hat\beta_{bill\_depth} x_{bill\_depth} +\hat\beta_{chinstrap} \\
\hat{y}_{gentoo} &= \hat\beta_0 + \hat\beta_{flipper\_length} x_{flipper\_length} + \hat\beta_{bill\_length} x_{bill\_length} + \hat\beta_{bill\_depth} x_{bill\_depth} +\hat\beta_{gentoo} 
\end{aligned}
$$
A Tukey-Anscombe plot to evaluate the fit of the model is generated below. No clear correlation can be seen between the residuals and fitted values, indicating that the expected value of the residual is 0. The variance of residuals does not appear to be significantly different for the fitted values either, suggesting heteroskedasticity is not present. 
```{r fig.dim = c(5,5)}
library(sfsmisc)
TA.plot( penguin.model.improved, fit = fitted(penguin.model.improved), res = residuals(penguin.model.improved), labels = "*" )
```


# Problem 3

Load and prepare data the problem 3

```{r}
# Load libraries
library(tidyverse)
library(class)
library(MASS)
library(palmerpenguins)
library(dplyr)
library(caret)
library(pROC)

# Load penguins data
Penguins <- penguins

# Add binary variable: if adelie
Penguins$adelie <- ifelse(Penguins$species == "Adelie", 1, 0)

# Extract just needed variables and remove na
Penguins_reduced <- Penguins %>% dplyr::select(body_mass_g, flipper_length_mm, adelie) %>% 
  mutate(body_mass_g = as.numeric(body_mass_g),
         flipper_length_mm = as.numeric(flipper_length_mm)) %>% 
  drop_na()

# Set seed for sample extraction
set.seed(4268)

# Find size of training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))

# Split data randomly into train and test
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)

train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

## a)

# i)

```{r}
# Make logistic regression model
model.LR <- glm(adelie ~ body_mass_g + flipper_length_mm,
                 family="binomial",
                 data=train)

# Get classification probabilities
pred.LR.prob <- model.LR %>% predict(test, type="response")

# Classify on test set using 0.5 cutoff
pred.LR <- ifelse(pred.LR.prob > 0.5, 1, 0)
```

# ii)

```{r}
# Make quadratic discriminant analysis model
model.QDA <- qda(adelie ~ body_mass_g + flipper_length_mm,
                 data=train)

# Get classification probabilities
pred.QDA.prob<- (model.QDA %>% predict(test))$posterior[0:nrow(test)]

# Classify on test set using 0.5 cutoff
pred.QDA <- ifelse(pred.QDA.prob > 0.5, 1, 0)
```

# iii)

```{r}
# K-nearest neighbor
model.KNN <- knn(train = train,
                 test = test,
                 cl = train$adelie,
                 k = 25,
                 prob = T)

# Get classification probabilities
pred.KNN.prob <- ifelse(model.KNN == 0, 1 - attributes(model.KNN)$prob, attributes(model.KNN)$prob)

# Classify on test set
pred.KNN <- model.KNN[0:nrow(test)]
```

# iv)

```{r}
# Calculate specificity and sensitivity
print("Sensitivity for logistic regression:")
sensitivity(as.factor(pred.LR), as.factor(test$adelie))
print("Sensitivity for QDA:")
sensitivity(as.factor(pred.QDA), as.factor(test$adelie))
print("Sensitivity for KNN:")
sensitivity(as.factor(pred.KNN), as.factor(test$adelie))

print("Specificity for logistic regression:")
specificity(as.factor(pred.LR), as.factor(test$adelie))
print("Specificity for QDA:")
specificity(as.factor(pred.QDA), as.factor(test$adelie))
print("Specificity for KNN:")
specificity(as.factor(pred.KNN), as.factor(test$adelie))

```

## b)

# i)

```{r}
# ROC for different classifiers
roc.LR <- roc(test$adelie, pred.LR.prob)
roc.KNN <- roc(test$adelie, pred.KNN.prob)
roc.QDA <- roc(test$adelie, pred.QDA.prob)

roc.list <- list(
  "Logistic regression" = roc.LR,
  "KNN" = roc.KNN,
  "QDA" = roc.QDA)

# Plot ROC
ggroc(roc.list, aes = "col",legacy.axes = TRUE) + 
  geom_abline() + 
  theme_classic() +
  ggtitle("ROC") +
  labs(x = "1 - Specificity",
       y = "Sensitivity",
       col = "Model type")

# AUC for the classifiers
roc.LR$auc
roc.KNN$auc
roc.QDA$auc
```

# ii)

The ROC curve shows how the binary classifiers are performing with a varying threshold. The plot shows the true positive rate, or sensitivity, against the true negative rate, or 1 - specificity. If a classifier is on the black diagonal line it performs just as good as guessing randomly and if it lays above the diagonal line it performs better. This means that a classifier is better the further up in the left corner its ROC curve are. In this case the logistic regression and the QDA performs better than the KNN classifier.

The AUC, or area under the curve, is just the area under the ROC curve and provides an aggregate measure of all classifiers for all thresholds. As expected, the AUC for the logistic regression and the QDA are higher than the one for the KNN indicating better performance.

# iii)

We would say the logistic regression model is the most interpretable model. This model provides easy to understand betas telling about the realtionship between the body mass and flipper length and if the penguin is Adelie or not. The two other models does not provide the same information about the data.

## c)

```{r}
model.LR$coefficients
```

Increasing the body mass of penguin with 1000g will result in the following change in the odds

$$
exp(0.0007120 * 1000) = 2.038
$$

The odds increases by a factor of 2.038. Alternative iii.

## d)

```{r}
# Get all data, from both test and training
penguins.pred.all <- Penguins_reduced %>% dplyr::select(-adelie)

# Classify on both test and training data using LR model
pred.all <- ifelse(model.LR %>% predict(penguins.pred.all) > 0.5, 1, 0)

penguins.pred.all <- Penguins_reduced %>% dplyr::mutate(pred = pred.all)

# Plot result
ggplot(penguins.pred.all, aes(x=flipper_length_mm, y=body_mass_g)) + 
  ggtitle("Classification of penguins") +
  geom_point(aes(col = as.factor(adelie), shape = as.factor(pred))) + 
  labs(x = "Flipper length [mm]",
       y = "Body mass [g]",
       col = "True class",
       shape = "Predicted class") + 
  scale_colour_discrete(labels=c("Not Adelie", "Adelie")) +
  scale_shape_discrete(labels=c("Not Adelie", "Adelie"))
```

# Problem 4

## a)
(i) True. The process for validation set approach is only necessary to be repeated once but 10 times in 10-fold CV.
(ii)False. LOOCV has the highest variance as the datasets used between training are highly correlated and differ only by one observation which can lead to high variance between completely new datasets.
(iii)False. In the validation set-approach the data is split randomly into 2 equal sets with 1 set being the training set and the other being the validation set. In the 2-fold CV the data is again randomly split into 2 equal parts but each data set take turn being the training and validation set.
(iv) False. LOOCV is the most computationally expensive way to do cross-validation. 

## b) 
```{r}

id <- "1chRpybM5cJn4Eow3-_xwDKPKyddL9M2N" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
```

```{r}
m <- glm(family="binomial", formula = chd ~ sbp + sex + smoking, data=d.chd)
d.chd.new <- data.frame(
  sex=as.integer(c(1)), 
  sbp = as.numeric(c(150)), 
  smoking=as.integer(c(0)))
pred <- predict(m, d.chd.new, type="response")
pred
```
The probability of chd for a non-smoking male with sbp=150 is 10%.

## c)
### 1)
```{r}
set.seed(4268)
library(boot)

prob <- function(df, index){
  m <- glm(family="binomial", subset=index, formula = chd ~ sbp + sex + smoking, data=d.chd)
  return(predict(m, d.chd.new, type="response")) 
}
```

### 2)
```{r}
B <- 1000
boot.result <- boot(d.chd, prob, B)
boot.result
```
From the bootstrapping method we observe an estimated standard error of 0.044.

### 3)
```{r}
boot.ci(boot.result, 0.95)
```
From the bootstrapping method we obtain [0.0107, 0.1843] as
the first order normal approximation of the 95 % CI.

### 4)
Since the 95 % CI is wide (nearly approaching zero in the left-hand limit)
we conclude based on the bootstrapping computations that the conditional
probability of chd in a non-smoking male with sbp=150 is quite uncertain.
Plausible values for conditional chd probability thus lie in the interval
[0.0107, 0.1843].
The upper limit of .1843 is perhaps most useful as it provides an upper 
97.5 % confidence bound on estimated conditional chd risk.

## d)
$$
\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
FALSE & FALSE & TRUE & TRUE\\
\end{matrix}
$$
