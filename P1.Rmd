---
title: "P1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1 (8P)  

We have a univariate continuous random variable $Y$ and a covariate $x$. Further, we have observed a training set of independent observation pairs $\{x_i, y_i\}$ for $i=1,\ldots,n$. Assume a regression model 
$$Y_i  = f(x_i) + \varepsilon_i \ ,$$ 
where $f$ is the true regression function, and $\varepsilon_i$ is an unobserved random variable with mean zero and constant variance $\sigma^2$ (not dependent on the covariate). Using the training set we can find an estimate of the regression function $f$, and we denote this by $\hat{f}$. We want to use $\hat{f}$ to make a prediction for a new observation (not dependent on the observations in the training set) at a covariate value $x_0$. The predicted response value is then $\hat{f}(x_0)$. We are interested in the error associated with this prediction.


## a) (2P)
Derive the decomposition of the expected test MSE, $E[y_0 - \hat{f}(x_0)]^2$, into three terms (bias, variance, and irreducible error).

### answer:
The expected MSE on the test set is given by:
$$\begin{aligned}
  E[(y_0 - \hat{f}(x_0))^2] &= 
  E[(f(x_0) - \hat{f}(x_0) + \epsilon)^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + 2E[\epsilon(f(x_0) - \hat{f}(x_0))] + E[\epsilon^2] \\
  &= E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0)] + E[\hat{f}(x_0)^2] + E[\epsilon^2]\\
  &= E[f(x_0)^2 - 2f(x_0)\hat{f}(x_0) + \hat{f}(x_0)^2] 
  + ( E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2) + E[\epsilon^2] \\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + ( E[\hat{f}(x_0)^2] - E[\hat{f}(x_0)]^2) + E[\epsilon^2]\\
  &= E[(f(x_0) - \hat{f}(x_0))^2] + Var[\hat{f}(x_0)] + Var[\epsilon]\\
  &= \text{Squared bias} + \text{Variance of prediction} + \text{Irreducible error}\\ 
\end{aligned}$$


## b) (1P)
Explain with words how we can interpret the three terms.

### answer:
The squared bias term represents the expected squared deviation between the prediction of the 
"true" model and the prediction of the fitted model.
The variance of prediction term represents the degree to which the prediction of
the fitted model can vary depending on the input. Higher variance of
prediction means the model can adapt it's prediction to input data to a greater extent
than a simpler model, implying that the model is more flexible. 
However, the increased "adaptability" may be unwanted if it leads to overfitting.
 

## c) (2P) - Multiple choice
Figure 1 shows the squared bias, variance, irreducible error and total error for increasing values of $K$ in KNN regression. Which of the following statements are true and which are false? Say for _each_ of them if it is true or false. 

  (i) Decreased $K$ corresponds to increased flexibility of the model.
  (ii) The variance increases with increased value of $K$.
  (iii) The blue line corresponds to the irreducible error.
  (iv) The squared bias decreases with increased value of $K$.


### Answer
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$

## d) (2P) Multiple choice
Which of the following statements are true and which are false? Say for _each_ of them if it is true or false.

(i) If the relationship between the predictors and response is highly non-linear, a flexible method will generally perform better than an inflexible method.
(ii) If the number of predictors $p$ is extremely large and the number of observations $n$ is small, a flexible method will generally perform better than an inflexible method.
(iii) In KNN classification, it is important to use the test set to select the value $K$, and not the training set, to avoid overfitting.
(iv) In a linear regression setting, adding more covariates will reduce the variance of the predictor function.
### Answer
$$\begin{matrix}
i & ii & iii & iv\\ 
\hline\\
TRUE & FALSE & TRUE & FALSE\\
\end{matrix}$$
 

## e) (1P) Single choice
$\mathbf{X} = [x_1, x_2, x_3]^T$ is a 3-dimensional random vector with covariance matrix
$$
\boldsymbol{\Sigma} = \begin{bmatrix}
  50 & 33 & 18 \\
  33 & 38 & -10 \\
  18 & -10  & 72
  \end{bmatrix}
$$

The correlation between element $x_1$ and $x_2$ of the vector $\mathbf{X}$ is:

  (i) 0.017
  (ii) -0.19
  (iii) 0.76
  (iv) 0.66
  (v) 0.10
  (vi) 0.3
  (vii) It is not possible to calculate the correlation, because this is not a proper covariance matrix.

### Answer
```{r}
library(matrixcalc)
mat <- cbind(c(50, 33, 18), c(33, 38, -10), c(18, -10, 72))
is.positive.semi.definite(mat)
```

Answer: iii: 0.76
